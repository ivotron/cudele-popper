\section{Implementation}
\label{sec:implementation}

% Outline of the section
Each section below corresponds ot the labeled arrows in
Figure~\ref{fig:decouple}. This implementation decouples policies from
mechanisms allowing applciations to choose the consistency and durability
semantics they need.

% Programmability
Of the 6 mechanisms in Figure~\ref{fig:decouple} 4 had to implemented and only
1 required changes to the underlying storage system itself.  RPCs and stream
can be achieved with tunables in configuration files for the storage system
(e.g., metadata cache size, logging on/off, etc.).  Persist", save, and create
are implemented as library and does not require modifications to the storage
system. Volatile apply requires changes to the metadata server to inject
updates back into the global namespace. 

\subsection{No Changes: RPCs, Stream}

RPCs is the default behavior of the storage system. Depending on the cache
state and configuration each operation incurrs at least RPC. For example, if a
client or metadata server is not caching the directory inode, all creates
within that directory will result in a lookup and a create request. If the
directory inode is cached then only the create needs to be sent.

Re-using the debugging and testing features in the storage system, we can turn
stream on and off. If it is off, then the metadata servers will not save
journals in the object store and the daemons that apply the journal to the
metadata store will never run. Performance numbers for enabling and disabling
the stream mechanism are presented back in Section~\S\ref{sec:durability}.

% General information about the journal
The journal segments are saved as objects in RADOS.  The journal has 4
pointers, described in `osdc/Journaler.h`:

\begin{itemize}
  \item write position: tail of the journal; points to the current session where we are appending events
  \item unused field: where someone is reading
  \item expire position: old journal segments
  \item trimmed position: where daemon is expiring old items
\end{itemize}

% How the journal tool works
Journal segments in RADDS have a header followed by serialized log events. The
log events are read by hopping over objects using the read offset and object
size pulled from the journal header.  After decoding them, we can examine the
metadata (1) about the event (e.g., type, timestamps, etc.) and (2) for inodes
that the event touches.

% The metablobs
The metadata for inodes that the event touches are called metadata blobs and
the ones associated with events are \textbf{unordered}; this layout makes
writing journal updates fast but the cost is realized when reading the metadata
blobs.  It makes sense to optimize for writing since reading only occurs on
failures. To reconstruct the namespace for the metadata blob, the journal tool
iterates over each metadata blob in the events and builds mappings of inodes to
directory entries (for directories) and parent inodes to child inodes/directory
entries.

\subsection{External Library: Create, Save, Persist}
\label{sec:external-library}
%Reads/Writes Journal Events}

% How it works
For ``create", ``save", and ``persist" Cudele provides a library for clients to
link into.  Recall that CephFS uses the object store (1) as a metadata store
for all information about files including the hierarchical namespace and (2) as
a staging area for the journal of updates before they are applied to the
metadata store. As discussed previously, the metadata store is optimized for
reading while the journal is optimized for writing.  Cudele re-uses the journal
tool to read/write log events to memory and persistent storage.

% creates
For create,
clients write to an in-memory journal and updates are merged into the global
namespace by replaying them onto the metadata store in the object store.  

% persist/save
For save, clients write serialized log events to a file on local disk and for
persist, clients store the file in the objects store. The overheads for save
and persist is the write bandwidth of the local disk and object store,
respectively.

% Level of intrusion
This required no changes to Ceph because the metadata server knows how to read
the events we were writing into the object store.  The client writes metadata
updates locally and merges the updates with the journal tool. The client that
decouples the namespace operates without any consistency and any conflicts at
the merge are resolved in favor of this client. Updates by other clients ({\it
i.e.} metadata writes to the global namespace) are overwritten.  We leverage
the journal tool's ability to reconstruct metadata events in memory. The client
library is shown in Figure~\ref{fig:decouple}.  Cudele adopts the following
process when an application decouples the namespace:

\begin{enumerate}
  \item ``\textbf{decoupled}": metadata server exports the journal events to a file
  \item ``\textbf{transfer}": the file is pulled by the client from the metadata server
  \item ``\textbf{snapshot}": client reads the file and materializes a snapshot of the
        namespace in memory
\end{enumerate}

% Why we re-use stuff
By using re-using the journal subsystem to implement the namespace decoupling,
Cudele leverages the write/read optimized data structures, the formats for
persisting events (similar to TableFS's SSTables), and the functions for
replaying events onto the internal namespace data structures.  

%Step 3 is the most complicated and requires understanding how the snapshot is
%materialized in memory. 
%
%\subsubsection{Operating on Snapshots} 
%
%Our first implementation attempted to re-create journal events using the same
%libraries that the metadata server uses. To construct a \texttt{mkdir} we tried
%to instantiate a Ceph inode and directory entry for the current file/dir and
%its parent.  This is too hard because there are too many moving parts in the
%metadata server (e.g., a mdlog class, stuff in memory, assumption that we can
%traverse up and down namespace, etc.). So when I tried add dentries and inodes
%it was trying to traverse up/down and it would almost always segfault when it
%was looking for something. These metablobs are supposed to be self container --
%the problem is I do not know what is supposed to go *inside* them. 
%
%Our second idea was to copy the metadata blog and change just what we needed.
%For example, we would save a binary dump of a generic \texttt{mkdir} event on
%disk. When the application makes a directory, this dump would be loaded and the
%fields would be changed before being written back to disk. Rather than
%traversing up and down a namespace in memory of a metadata server, we should
%traverse up and down the namespace *inside* the metadata blob. This
%implementation requires disk IO and editing the log event is non-trivial for
%two reasons:
%
%\begin{itemize}
%
%  \item methods do not edit events; they just write them
%
%  \item the metadata that the event touches (e.g., the metablob) is unorganized
%  on disk for performance -- it is trade-off for writing data faster serially and
%  reconstructing information slowly since failure is not considered the norm
%
%\end{itemize}
%
%Faced with these challenges we landed on our final implementation: load the
%snapshot into the the data structures used to examine and replay journals, edit
%those data structures, and write them out to disk as binary.

\subsection{Storage System Changes: Volatile Apply}

% - how it makes no gaurantees
The volatile apply mechanism ({\it i.e.} the ``v\_apply" arrow in
Figure~\ref{fig:decouple}) takes an in-memory journal on the client and applies
the updates directly to the in-memory namespace maintained by the metadata
servers. We say volatile because Cudele makes no consistency or durability
guarantees. If a concurrent update from a client occurs there is no
rule for resolving conflicts and if the client or metadata server crashes there
may be no way to recover. Relaxing these constraints gives volatile apply the
best performance.

% difference between apply and volatile apply
In contrast, apply uses the the object store to merge the journal of updates
from the client to the metadata server. Apply is safer than volatile apply but
has a performance overhead because objects in the metadata store need to be
read from and written back to the object store.  Apply was already implemented
by the journal tool: it reads the journal from the object store, adds events to
the journal, and writes the metadata updates out to the metadata store in the
object store. Cudele's volatile apply executes ``save" (described
in~\S\ref{sec:external-library}), transfers the file of metadata updates to the
metadata server, and then merges the updates directly into the in-memory
metadata store of the metadata cluster.

% Why so slow?
Creating many files in the same directory would touch the same object but the
existing implementation results in this object being repeatedly pushed/pulled.

% - how it actually works
% - bugs I fixed in the last couple of commits

%The metadata objects are located with naming schemes (200.000* for journal
%objects and 1.inode for metadata storage objects). 

% How it works: socket for changing daemon's internal state (debugging, logging, behaviour)
% 1. API for putting state into the daemon dynamically
% 2. Hooks directly into daemon code so we can use any parsing functionality in there
% 3. Documentation all the tunables

% 1. read journal of updates from file
% 2. call replay (uses same code as when an MDS comes back) on each event
% 3. skip inodes so MDS doesn't hand out those new inodes


