\section{Implementation}
\label{sec:implementation}

% Outline of the section
Each section below corresponds to the labeled arrows in
Figure~\ref{fig:decouple}. This implementation decouples policies from
mechanisms allowing applications to choose the consistency and durability
semantics they need.

% Programmability
Of the 6 mechanisms in Figure~\ref{fig:decouple} 4 had to implemented and only
1 required changes to the underlying storage system itself.  ``RPCs" and
``Stream" can be achieved with configuration settings 
.  ``Global Persist", ``Local Persist",
``Nonvolatile Apply", and ``Create" are implemented as a library and they do
not require modifications to the storage system. ``Volatile Apply" requires
changes to the metadata server to inject updates back into the global
namespace. 

\subsection{No Changes: RPCs, Stream}
\label{sec:no-changes-rpcs-stream}

RPCs is the default behavior of CephFS.  Using existing configuration settings
({\it e.g.}, metadata cache size, logging on/off, etc.) in Ceph we can turn
``Stream" on and off.  If it is off, then the metadata servers will not save
journals in the object store and the daemons that apply the journal to the
metadata store will never run. 

% General information about the journal
%The journal segments are saved as objects in RADOS.  The journal has 4
%pointers, described in `osdc/Journaler.h`:
%
%\begin{itemize}
%  \item write position: tail of the journal; points to the current session where we are appending events
%  \item unused field: where someone is reading
%  \item expire position: old journal segments
%  \item trimmed position: where daemon is expiring old items
%\end{itemize}
%
%% How the journal tool works
%Journal segments in RADDS have a header followed by serialized log events. The
%log events are read by hopping over objects using the read offset and object
%size pulled from the journal header.  After decoding them, we can examine the
%metadata (1) about the event (e.g., type, timestamps, etc.) and (2) for inodes
%that the event touches.
%
%% The metablobs
%The metadata for inodes that the event touches are called metadata blobs and
%the ones associated with events are \textbf{unordered}; this layout makes
%writing journal updates fast but the cost is realized when reading the metadata
%blobs.  It makes sense to optimize for writing since reading only occurs on
%failures. To reconstruct the namespace for the metadata blob, the journal tool
%iterates over each metadata blob in the events and builds mappings of inodes to
%directory entries (for directories) and parent inodes to child inodes/directory
%entries.

\subsection{Library: Create, Local/Global Persist, Nonvolatile Apply}
\label{sec:library-create-local-global-persist}
%Reads/Writes Journal Events}

% How it works
For these mechanisms, Cudele provides a library for clients to link into.  For
``Create", clients write to an in-memory journal and updates are merged into
the global namespace by replaying them onto the metadata store either in the
metadata servers or the object store.  For ``Local Persist", clients write
serialized log events to a file on local disk and for ``Global Persist",
clients push the journal into the objects store. The overheads for both ``Local
Persist" and ``Global Persist" is the write bandwidth of the local disk and
object store, respectively. Finally, for ``Nonvolatile Apply" the journal
updates are replayed onto the metadata store in the object store and the
metadata servers are restarted.

% Level of intrusion
These implementations required no changes to Ceph because the metadata servers
know how to read the events the library is writing into the object store.  By
re-using the journal subsystem to implement the namespace decoupling, Cudele
leverages the write/read optimized data structures, the formats for persisting
events (similar to TableFS's SSTables~\cite{ren:atc2013-tablefs}), and the
functions for replaying events onto the internal namespace data structures.  

% Why we re-use stuff
%Step 3 is the most complicated and requires understanding how the snapshot is
%materialized in memory. 
%
%\subsubsection{Operating on Snapshots} 
%
%Our first implementation attempted to re-create journal events using the same
%libraries that the metadata server uses. To construct a \texttt{mkdir} we tried
%to instantiate a Ceph inode and directory entry for the current file/dir and
%its parent.  This is too hard because there are too many moving parts in the
%metadata server (e.g., a mdlog class, stuff in memory, assumption that we can
%traverse up and down namespace, etc.). So when I tried add dentries and inodes
%it was trying to traverse up/down and it would almost always segfault when it
%was looking for something. These metablobs are supposed to be self container --
%the problem is I do not know what is supposed to go *inside* them. 
%
%Our second idea was to copy the metadata blog and change just what we needed.
%For example, we would save a binary dump of a generic \texttt{mkdir} event on
%disk. When the application makes a directory, this dump would be loaded and the
%fields would be changed before being written back to disk. Rather than
%traversing up and down a namespace in memory of a metadata server, we should
%traverse up and down the namespace *inside* the metadata blob. This
%implementation requires disk IO and editing the log event is non-trivial for
%two reasons:
%
%\begin{itemize}
%
%  \item methods do not edit events; they just write them
%
%  \item the metadata that the event touches (e.g., the metablob) is unorganized
%  on disk for performance -- it is trade-off for writing data faster serially and
%  reconstructing information slowly since failure is not considered the norm
%
%\end{itemize}
%
%Faced with these challenges we landed on our final implementation: load the
%snapshot into the the data structures used to examine and replay journals, edit
%those data structures, and write them out to disk as binary.

\subsection{Storage System Changes: Volatile Apply}

% - how it makes no gaurantees
The ``Volatile Apply" mechanism takes an in-memory journal on the client and
applies the updates directly to the in-memory namespace maintained by the
metadata servers. We say volatile because -- in exchange for peak performance
-- Cudele makes no consistency or durability guarantees while the mechanism is
executing.  If a concurrent update from a client occurs there is no rule for
resolving conflicts and if the client or metadata server crashes there may be
no way to recover. 

% difference between apply and volatile apply
In contrast, ``Nonvolatile Apply" uses the the object store to merge the journal of updates
from the client to the metadata server. ``Nonvolatile Apply" is safer but
has a performance overhead because objects in the metadata store need to be
read from and written back to the object store.  ``Volatile Apply" was already implemented
by the journal tool: it reads the journal from the object store, adds events to
the journal, and writes the metadata updates out to the metadata store in the
object store. 

% - how it actually works
% - bugs I fixed in the last couple of commits

%The metadata objects are located with naming schemes (200.000* for journal
%objects and 1.inode for metadata storage objects). 

% How it works: socket for changing daemon's internal state (debugging, logging, behaviour)
% 1. API for putting state into the daemon dynamically
% 2. Hooks directly into daemon code so we can use any parsing functionality in there
% 3. Documentation all the tunables

% 1. read journal of updates from file
% 2. call replay (uses same code as when an metadata server comes back) on each event
% 3. skip inodes so metadata server doesn't hand out those new inodes


